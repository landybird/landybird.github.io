---
title: filebeat采集日志
description: filebeat 采集数据  kafka-python 脚本消费数据入库
categories: 
- python    
tags:
- kafka   
---

### Filebeat 采集日志

#### [Filebeat](https://www.elastic.co/cn/beats/filebeat)


`Filebeat`是用于转发和集中日志数据的轻量级传送工具。
`Filebeat`监视您指定的日志文件或位置，收集日志事件，并将它们转发到`Elasticsearch`或`Logstash`进行索引。

`Filebeat`由两个主要组件组成：`inputs` 和  `harvesters` （直译：收割机，采集器）。
这些组件一起工作以跟踪文件，并将事件数据发送到你指定的输出
    
    harvester 逐行读取每个文件（一行一行地读取每个文件），并把这些内容发送到输出。
    每个文件启动一个harvester
    
    一个input 负责管理harvesters，并找到所有要读取的源。
    如果input类型是log，则input查找驱动器上与已定义的glob路径匹配的所有文件，并为每个文件启动一个harvester
    

[log input 配置](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-log.html)


```yaml

filebeat.inputs:


- type: log

  paths:
    - \path\to\logs\*.log
  multiline:
    # 时间格式开头的日志
    pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}'
    negate: true
    match: after


output.kafka:
  # Array of hosts to connect to.
  hosts: ["localhost:9092"]
  enabled: true
  topic: topic
```

启动 
    
    # 日志采集到kafka
    filebeat -e -c filebeat.yml
    
        -c：配置文件位置
        -path.logs：日志位置
        -path.data：数据位置
        -path.home：家位置
        -e：关闭日志输出
        -d 选择器：启用对指定选择器的调试。 对于选择器，可以指定逗号分隔的组件列表，也可以使用-d“*”为所有组件启用调试.例如，-d“publish”显示所有“publish”相关的消息。


    # consumer 获取日志
    kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic topic --from-beginning


#### 配置多个`kafka topic`

- 多`filebeat`实例，每个`filebeat`实例设置一个`topic`


- 一`filebeat`实例多`topic`
    
    
```yaml

 filebeat.inputs:


- type: log

  # Change to true to enable this input configuration.
  enabled: true
  fields:
    kafka_topic: ber_click
  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /home/to/logs/click.log*


- type: log

  # Change to true to enable this input configuration.
  enabled: true
  fields:
    kafka_topic: ber_show

  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /home/to/logs/show.log*


output.kafka:
  # Array of hosts to connect to.
  hosts: ["xxx:9093", "xxx:9093", "xxx:9093"]
  enabled: true
  topic: '%{[fields.kafka_topic]}'

```

使用 [`filestream input`](https://www.elastic.co/guide/en/beats/filebeat/7.14/filebeat-input-filestream.html)替代 `log` with ` Elastic 7.14+`



### kafka 

`Kafka`是最初由`Linkedin`公司开发，是一个分布式、分区的、多副本的、多订阅者，基于`zookeeper`协调的`分布式日志系统`（也可以当做MQ系统）

常见可以用于web/nginx日志、访问日志，消息服务等等

`Linkedin`于2010年贡献给了Apache基金会并成为顶级开源项目


[kafka下载](https://www.apache.org/dyn/closer.cgi?path=/kafka/2.8.0/kafka_2.12-2.8.0.tgz)   
 
    windows 下 
    
        # 启动 zookeeper
        zookeeper-server-start.bat  ..\..\config\zookeeper.properties
        
        # 启动 Kafka
        kafka-server-start.bat  ..\..\config\server.properties
        
        # 创建一个主题
        kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic kafka-test-topic
        
        # 查看创建的主题列表
        kafka-topics.bat --list --zookeeper localhost:2181
        

#### [`kafka-python API`](https://kafka-python.readthedocs.io/en/master/index.html)

- `多个进程重复消费数据`

因此假设进程A正在消费分区1的信息，并提交了偏移量，
之后又消费了10条数据，还没来得及提交偏移量的时候，
`reblance机制`让进程B来继续消费分区1的信息，
此时进程B会从上次进程A提交偏移量的地方开始消费，
因此这10条数据就是重复消费的

解决方式:

    将消费者进程与分区静态绑定
    
    self.tp = TopicPartition(topic=self.topic, partition=settings.KAFKA_CONSUMER_PARTITION)
    self.consumer.assign([self.tp])
    
- `消费组实现容错性机制`

一个有2个partition的topic，和2个consumer，这2个consumer共同消费同一个topic中的数据
两个consumer同时运行的情况下，它们分别消费不同partition中的数据。

consumer1消费partition 0中的数据，consumer2消费parition 1中的数据。
刚开始consumer2只消费partition1中的数据，当consumer1退出后， consumer2中也开始消费partition 0中的数据了

- 同一`group_id`的consumer

消费者将继续为特定group自动使用最后一个偏移量的数据

